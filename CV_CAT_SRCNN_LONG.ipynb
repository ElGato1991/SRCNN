{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebc1f16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\adria\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: packaging in c:\\users\\adria\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kagglehub) (24.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\adria\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kagglehub) (6.0.3)\n",
      "Requirement already satisfied: requests in c:\\users\\adria\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kagglehub) (2.32.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\adria\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\adria\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->kagglehub) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adria\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\adria\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->kagglehub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adria\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->kagglehub) (2025.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\adria\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "c:\\Users\\adria\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\adria\\.cache\\kagglehub\\datasets\\adile45\\coco-cat-subset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "%pip install kagglehub\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"adile45/coco-cat-subset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e20b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "class CatSuperRes(Dataset):\n",
    "    def __init__(self, root_dir, split=\"train\"):\n",
    "        print(f\"Initializing dataset (split={split})...\")\n",
    "        # Suche rekursiv nach Bildern\n",
    "        self.image_paths = []\n",
    "        # Suche nach gängigen Bildformaten\n",
    "        for ext in ['*.jpg', '*.jpeg', '*.png']:\n",
    "            self.image_paths.extend(glob.glob(os.path.join(root_dir, '**', ext), recursive=True))\n",
    "        \n",
    "        self.image_paths.sort()\n",
    "        \n",
    "        # Split 60/40 (train/val)\n",
    "        split_idx = int(0.6 * len(self.image_paths))\n",
    "        if split == \"train\":\n",
    "            self.image_paths = self.image_paths[:split_idx]\n",
    "        else:  # val\n",
    "            self.image_paths = self.image_paths[split_idx:]\n",
    "            \n",
    "        print(f\"  -> Found {len(self.image_paths)} images for {split}.\")\n",
    "\n",
    "        # Transform: Erst auf eine Mindestgröße bringen, dann Random Crop für High-Res Details\n",
    "        self.target_size = 256\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            # Falls ein Bild kleiner als 256 ist, vergrößern wir die kurze Seite auf 256\n",
    "            transforms.Resize(self.target_size), \n",
    "            # Wir schneiden ein zufälliges 256x256 Stück aus (erhält Originaldetails besser als squashing)\n",
    "            transforms.RandomCrop((self.target_size, self.target_size)),\n",
    "            # Data Augmentation: Random Flip\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        # Downsampling mit Resize (Bicubic) statt MaxPool\n",
    "        self.downsample = transforms.Resize((128, 128), interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Debug output (throttled) - zeigt Aktivität, ohne zu fluten\n",
    "        # Da shuffle=True ist, kommen die Indizes zufällig.\n",
    "        if idx % 200 == 0:\n",
    "             print(f\"  [Dataset] Loading image index {idx} (sample check)\")\n",
    "\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\") # Sicherstellen, dass es RGB ist\n",
    "        \n",
    "        high_res = self.transform(img)      # (3, 256, 256)\n",
    "        low_res = self.downsample(high_res) # (3, 128, 128)\n",
    "        \n",
    "        return low_res, high_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4903da1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up datasets...\n",
      "Initializing dataset (split=train)...\n",
      "  -> Found 2578 images for train.\n",
      "Initializing dataset (split=val)...\n",
      "  -> Found 1720 images for val.\n",
      "Setting up DataLoaders...\n",
      "DataLoaders ready.\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up datasets...\")\n",
    "train_ds = CatSuperRes(root_dir=path, split=\"train\")\n",
    "val_ds   = CatSuperRes(root_dir=path, split=\"val\")\n",
    "\n",
    "print(\"Setting up DataLoaders...\")\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
    "print(\"DataLoaders ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab2f709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual\n",
    "        return out\n",
    "\n",
    "class ResNetSR(nn.Module):\n",
    "    def __init__(self, num_res_blocks=4, channels=64):\n",
    "        super(ResNetSR, self).__init__()\n",
    "        \n",
    "        # Initial Feature Extraction\n",
    "        self.conv1 = nn.Conv2d(3, channels, kernel_size=9, padding=4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Residual Blocks\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(channels) for _ in range(num_res_blocks)]\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        \n",
    "        # Upsampling (PixelShuffle)\n",
    "        # 128 -> 256 (2x upsampling)\n",
    "        self.conv3 = nn.Conv2d(channels, channels * 4, kernel_size=3, padding=1)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Final Output\n",
    "        self.conv4 = nn.Conv2d(channels, 3, kernel_size=9, padding=4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.relu(self.conv1(x))\n",
    "        out = self.res_blocks(out1)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += out1 # Global Skip Connection\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.pixel_shuffle(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        out = self.conv4(out)\n",
    "        out = torch.sigmoid(out) # Output in [0, 1]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ed0e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Preparing visualization batch...\n",
      "  [Dataset] Loading image index 0 (sample check)\n",
      "Visualization batch ready.\n",
      "Starting training loop...\n",
      "Epoch 1/15 - Training phase started. Waiting for first batch...\n",
      "Visualization batch ready.\n",
      "Starting training loop...\n",
      "Epoch 1/15 - Training phase started. Waiting for first batch...\n",
      "  [Epoch 1] First batch loaded! Processing...\n",
      "  [Epoch 1] First batch loaded! Processing...\n",
      "  [Epoch 1] Train Batch 1/41 processed\n",
      "  [Epoch 1] Train Batch 1/41 processed\n",
      "  [Dataset] Loading image index 800 (sample check)\n",
      "  [Dataset] Loading image index 800 (sample check)\n",
      "  [Epoch 1] Train Batch 2/41 processed\n",
      "  [Epoch 1] Train Batch 2/41 processed\n",
      "  [Epoch 1] Train Batch 3/41 processed\n",
      "  [Epoch 1] Train Batch 3/41 processed\n",
      "  [Epoch 1] Train Batch 4/41 processed\n",
      "  [Epoch 1] Train Batch 4/41 processed\n",
      "  [Epoch 1] Train Batch 5/41 processed\n",
      "  [Epoch 1] Train Batch 5/41 processed\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = ResNetSR().to(device)\n",
    "criterion = nn.L1Loss() # L1 Loss ist oft besser für Schärfe als MSE\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "print(\"Preparing visualization batch...\")\n",
    "# Ein festes Bild für die Visualisierung auswählen (z.B. das erste aus dem Validation Loader)\n",
    "# Wir nehmen einen Batch und picken das erste Bild heraus\n",
    "fixed_low_batch, fixed_high_batch = next(iter(val_loader))\n",
    "fixed_low_img = fixed_low_batch[0].unsqueeze(0).to(device)   # (1, 3, 128, 128)\n",
    "fixed_high_img = fixed_high_batch[0].unsqueeze(0).to(device) # (1, 3, 256, 256)\n",
    "print(\"Visualization batch ready.\")\n",
    "\n",
    "print(\"Starting training loop...\")\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Training phase started. Waiting for first batch...\")\n",
    "    for batch_idx, (low_res, high_res) in enumerate(train_loader):\n",
    "        # Sobald wir hier sind, wurden die Daten für den Batch geladen\n",
    "        if batch_idx == 0:\n",
    "            print(f\"  [Epoch {epoch+1}] First batch loaded! Processing...\")\n",
    "\n",
    "        low_res  = low_res.to(device)\n",
    "        high_res = high_res.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(low_res)\n",
    "        loss = criterion(outputs, high_res)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * low_res.size(0)\n",
    "        \n",
    "        # Print status frequently at the beginning, then every 10 batches\n",
    "        if batch_idx < 5 or (batch_idx + 1) % 10 == 0:\n",
    "             print(f\"  [Epoch {epoch+1}] Train Batch {batch_idx+1}/{len(train_loader)} processed\")\n",
    "\n",
    "    scheduler.step()\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Validation phase started\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (low_res, high_res) in enumerate(val_loader):\n",
    "            low_res  = low_res.to(device)\n",
    "            high_res = high_res.to(device)\n",
    "            \n",
    "            outputs = model(low_res)\n",
    "            loss = criterion(outputs, high_res)\n",
    "            \n",
    "            val_loss += loss.item() * low_res.size(0)\n",
    "            \n",
    "            # Print status every 10 batches\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"  [Epoch {epoch+1}] Val Batch {batch_idx+1}/{len(val_loader)} processed\")\n",
    "    \n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    epoch_duration = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} Finished in {epoch_duration:.2f}s. LR: {current_lr:.6f}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # --- Visualisierung ---\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Visualizing result...\")\n",
    "    with torch.no_grad():\n",
    "        pred_img = model(fixed_low_img)\n",
    "    \n",
    "    # Umwandeln für Matplotlib (Tensor -> Numpy, CHW -> HWC)\n",
    "    lr_disp = fixed_low_img[0].permute(1, 2, 0).cpu().numpy()\n",
    "    hr_disp = fixed_high_img[0].permute(1, 2, 0).cpu().numpy()\n",
    "    pr_disp = pred_img[0].permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1); plt.title(\"Low-Res Input\"); plt.imshow(lr_disp); plt.axis(\"off\")\n",
    "    plt.subplot(1, 3, 2); plt.title(\"High-Res Target\"); plt.imshow(hr_disp); plt.axis(\"off\")\n",
    "    plt.subplot(1, 3, 3); plt.title(f\"Prediction (Epoch {epoch+1})\"); plt.imshow(pr_disp); plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729728b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "low_res, high_res = next(iter(val_loader))\n",
    "low_res  = low_res.to(device)\n",
    "high_res = high_res.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = model(low_res)\n",
    "\n",
    "for i in range(15):\n",
    "    # Permute für Matplotlib (C, H, W) -> (H, W, C)\n",
    "    lr  = low_res[i].permute(1, 2, 0).cpu().numpy()\n",
    "    hr  = high_res[i].permute(1, 2, 0).cpu().numpy()\n",
    "    pr  = preds[i].permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(9,3))\n",
    "    plt.subplot(1,3,1); plt.title(\"Low-Res\");  plt.imshow(lr); plt.axis(\"off\")\n",
    "    plt.subplot(1,3,2); plt.title(\"High-Res GT\"); plt.imshow(hr); plt.axis(\"off\")\n",
    "    plt.subplot(1,3,3); plt.title(\"ISR Pred\"); plt.imshow(pr); plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
